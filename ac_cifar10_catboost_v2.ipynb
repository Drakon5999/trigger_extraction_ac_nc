{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c831a-4d84-4480-9596-541ef21cd290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import importlib\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.utils.data\n",
    "import json\n",
    "from tdc_starter_kit import utils\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Subset\n",
    "seed = 77\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import activation_clustering_features\n",
    "importlib.reload(activation_clustering_features)\n",
    "ActivationClustering = activation_clustering_features.ActivationClustering\n",
    "\n",
    "import diploma_utils\n",
    "importlib.reload(diploma_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to poisoned Trojan detection challenge dataset\n",
    "poisoned_path = \"/root/poisoned_models/datasets/tdc_datasets/detection/train/trojan/id-{}/{}\"\n",
    "\n",
    "specifications, infos = diploma_utils.load_specs(poisoned_path)\n",
    "\n",
    "keys = diploma_utils.filter_by_dataset('CIFAR-10', infos)\n",
    "specifications = {key: specifications[key] for key in keys}\n",
    "infos = {key: infos[key] for key in keys}\n",
    "\n",
    "models = diploma_utils.load_models(keys, poisoned_path)\n",
    "print(f\"{len(models)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc12b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = infos[keys[0]]\n",
    "\n",
    "clean_dataset, test_dataset, num_classes = utils.load_data(info[\"dataset\"], folder=\"/root/datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942fd6b-0859-4bf5-b838-02e3ac819c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3be7359e",
   "metadata": {},
   "source": [
    "Выведем примеры изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 89\n",
    "info = infos[keys[k]]\n",
    "model = models[keys[k]]\n",
    "attack_specification = specifications[keys[k]]\n",
    "\n",
    "print(json.dumps(info, indent=4))\n",
    "\n",
    "img = clean_dataset[0][0]\n",
    "# add trigger to image\n",
    "img_with_trigger, _ = utils.insert_trigger(img, attack_specification)\n",
    "print(model(img_with_trigger.unsqueeze(0)).argmax())\n",
    "print(model(img.unsqueeze(0)).argmax())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=6, figsize=(16, 8))\n",
    "\n",
    "for i in range(6):\n",
    "    # First visualize an image without the trigger and with the trigger\n",
    "    img = clean_dataset[i][0].unsqueeze(0)\n",
    "    attack_specification = attack_specification\n",
    "    img_with_trigger, _ = utils.insert_trigger(img, attack_specification)\n",
    "    ax[0, i].imshow(img.squeeze(0).permute(1,2,0).numpy())\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(img_with_trigger.squeeze(0).permute(1,2,0).numpy())\n",
    "    ax[1, i].axis('off')\n",
    "    # Now visualize another image with the same trigger\n",
    "    img = clean_dataset[100+i][0].unsqueeze(0)\n",
    "    img_with_trigger, _ = utils.insert_trigger(img, attack_specification)\n",
    "    ax[2, i].imshow(img_with_trigger.squeeze(0).permute(1,2,0).numpy())\n",
    "    ax[2, i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1a0667a",
   "metadata": {},
   "source": [
    "## Побробуем собрать датасет при помощи кластеризации активаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886cdfa-c7bc-4a57-9c53-b2b0c4111cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_by_trig_type = {\n",
    "    \"patch\": diploma_utils.filter_by_trigger_type(\"patch\", infos),\n",
    "    \"blended\": diploma_utils.filter_by_trigger_type(\"blended\", infos)\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ba089",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "num_epochs = 10  # for training\n",
    "number_of_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849b103-7241-4a0d-9ea8-488f5414438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduction_method = 'FastICA'\n",
    "nb_clusters = 6\n",
    "nb_dims = 12\n",
    "silhouette_threshold = 0.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc75b40-1a8b-4454-8555-dd3efef2e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import pickle\n",
    "import catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77bd2f-e998-4e11-b8e8-d8d6ba067e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9221e42-a768-4895-8906-9810f9fa8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"cifar10_train.pd.csv\", index_col=0)\n",
    "# test_df = pd.read_csv(\"cifar10_test.pd.csv\", index_col=0)\n",
    "# train_lables_1d = np.load(\"cifar10_train_lables_1d.np.npy\")\n",
    "# test_lables_1d = np.load(\"cifar10_test_lables_1d.np.npy\")\n",
    "\n",
    "# with open('cifar10_train_keys.pkl', 'rb') as f:\n",
    "#     train_keys = pickle.load(f)\n",
    "    \n",
    "# with open('cifar10_test_keys.pkl', 'rb') as f:\n",
    "#     test_keys = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c5e62-1712-4235-bb92-ea432e0e042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys, test_keys = train_test_split(keys_by_trig_type[\"patch\"])\n",
    "train_keys_b, test_keys_b = train_test_split(keys_by_trig_type[\"blended\"])\n",
    "train_keys += train_keys_b\n",
    "test_keys += test_keys_b\n",
    "\n",
    "with open('cifar10_train_keys.pkl', 'wb') as f:\n",
    "    pickle.dump(train_keys, f)\n",
    "    \n",
    "with open('cifar10_test_keys.pkl', 'wb') as f:\n",
    "    pickle.dump(test_keys, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb8190-ddbe-4521-ac1d-121f63b98185",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"key\", \"nb_classes\", \"nb_dims\", \"nb_clusters\", \"image_size\"]\n",
    "c2i = {col: i for i, col in enumerate(columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82d925-a10d-4f7a-a175-348284fd069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dataframe(key, ac_result):\n",
    "    df = np.empty((len(clean_dataset), len(columns)))\n",
    "    df[:, c2i[\"key\"]] = key\n",
    "    df[:, c2i[\"nb_classes\"]] = number_of_classes\n",
    "    df[:, c2i[\"nb_dims\"]] = nb_dims\n",
    "    df[:, c2i[\"nb_clusters\"]] = nb_clusters\n",
    "    df[:, c2i[\"image_size\"]] = np.prod(clean_dataset[0][0].shape)\n",
    "\n",
    "    pdf = pd.DataFrame(df, columns=columns)\n",
    "    return pdf.assign(**ac_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d2256-f14a-4504-a9e1-eda2c2ea2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef4b16-2c63-4285-98d4-9c16d9d07822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc_starter_kit.wrn import WideResNet\n",
    "type(model).__name__ == \"WideResNet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c23cc7-10a7-4221-a83f-ecb6e0c17e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "\n",
    "train_lables = []\n",
    "test_lables = []\n",
    "fm = []\n",
    "reduced_fm = []\n",
    "for t in ['train', 'test']:\n",
    "    for key in tqdm(train_keys if t == 'train' else test_keys, leave=False):\n",
    "        dataset, result = diploma_utils.get_ac_result(models[key], specifications[key], clean_dataset)\n",
    "        fm.append(result['all_fm'])\n",
    "        reduced_fm.append(result['all_reduced_fm'])\n",
    "        diploma_utils.fix_features(result)\n",
    "        pdf = fill_dataframe(key, result)\n",
    "        cur_lables = np.zeros(len(dataset,))\n",
    "        cur_lables[dataset.poisoned_indices] = 1\n",
    "        \n",
    "        if t == 'train':\n",
    "            if train_df is not None:\n",
    "                train_df = pd.concat([train_df, pdf], ignore_index=True, copy=False)\n",
    "            else:\n",
    "                train_df = pdf\n",
    "                \n",
    "            train_lables.append(cur_lables)\n",
    "        else:\n",
    "            if test_df is not None:\n",
    "                test_df = pd.concat([test_df, pdf], ignore_index=True, copy=False)\n",
    "            else:\n",
    "                test_df = pdf\n",
    "                \n",
    "            test_lables.append(cur_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bebbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сконкатинируем целевые лейблы в один numpy массив\n",
    "train_lables_1d = np.concatenate(train_lables)\n",
    "test_lables_1d = np.concatenate(test_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635d3c8-8034-4958-a3e9-4292dca79803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_poisoned_dataset(clean_dataset, attack_specification, poisoned_indices=None):\n",
    "    if attack_specification is None:\n",
    "        return clean_dataset\n",
    "\n",
    "    poisoned_dataset = utils.PoisonedDataset(clean_dataset, attack_specification)\n",
    "\n",
    "    if poisoned_indices is not None:\n",
    "        poisoned_dataset.poisoned_indices = poisoned_indices\n",
    "\n",
    "    return poisoned_dataset\n",
    "\n",
    "def get_min_img_dist_to_cluster_means_feature(\n",
    "    keys,\n",
    "    df,\n",
    "    original_is_poisoned,\n",
    "    clean_dataset,\n",
    "    specifications,\n",
    "    batch_size=1000,\n",
    "    num_workers=0    \n",
    "):\n",
    "    # original_lables may be poisoned if dataset is\n",
    "    all_feature = []\n",
    "    image_shape = clean_dataset[0][0].shape\n",
    "    for key in tqdm(keys):\n",
    "        attack_specification = specifications[key]\n",
    "        poisoned_dataset = get_poisoned_dataset(\n",
    "            clean_dataset,\n",
    "            attack_specification,\n",
    "            poisoned_indices=original_is_poisoned[df.key == int(key)].nonzero()[0])\n",
    "\n",
    "        class_cluster_to_id : dict[tuple[int, int], int] = {} \n",
    "        cluster_means_images = torch.zeros((num_classes * nb_clusters, *image_shape))\n",
    "        \n",
    "\n",
    "        # получаем кластера для каждого класса и усредняем их\n",
    "        cur_key_dataset = df[df.key == int(key)]\n",
    "        for target_class in range(num_classes):\n",
    "            for cluster_i in range(nb_clusters):\n",
    "                images_for_cluster_idx = (\n",
    "                    (cur_key_dataset.all_pred_label == target_class) \n",
    "                    & (cur_key_dataset.all_clusters == cluster_i)).values.nonzero()[0]\n",
    "                \n",
    "                dev_dataloader = torch.utils.data.DataLoader(\n",
    "                    Subset(poisoned_dataset, images_for_cluster_idx),\n",
    "                    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "                mean_image = torch.zeros(image_shape)\n",
    "                for images_set, _ in dev_dataloader:\n",
    "                    mean_image += images_set.sum(dim=0)\n",
    "                mean_image = mean_image / images_for_cluster_idx.shape[0]\n",
    "                ind = target_class * nb_clusters + cluster_i\n",
    "                class_cluster_to_id[(target_class, cluster_i)] = ind\n",
    "                cluster_means_images[ind] = mean_image\n",
    "\n",
    "        dev_dataloader = torch.utils.data.DataLoader(\n",
    "            poisoned_dataset,\n",
    "            batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        cur_key_dataset_val = cur_key_dataset.loc[:, [\"all_pred_label\", \"all_clusters\"]].values\n",
    "        \n",
    "        for i, (images_set, _) in enumerate(dev_dataloader):\n",
    "            img_infos = cur_key_dataset_val[i * batch_size : i * batch_size + len(images_set)]\n",
    "            min_dists = torch.cdist(images_set.flatten(1), cluster_means_images.flatten(1), p=1)\n",
    "            # global md\n",
    "            # global ind_md\n",
    "            # md = min_dists\n",
    "            # ind_md = np.column_stack(\n",
    "                    # (np.arange(0, len(img_infos)), (img_infos[:, 0] * nb_clusters + img_infos[:, 1]))\n",
    "                # )\n",
    "            # return\n",
    "            min_dists[\n",
    "                np.arange(0, len(img_infos)), (img_infos[:, 0] * nb_clusters + img_infos[:, 1])\n",
    "            ] = torch.inf\n",
    "            min_dists = torch.nan_to_num(min_dists, nan=torch.inf)\n",
    "            all_feature.append(min_dists.min(dim=1).values.numpy())\n",
    "\n",
    "    return np.concatenate(all_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e305ba-77d3-41f6-ac09-c5a893364528",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_img_dist_to_cluster_means = get_min_img_dist_to_cluster_means_feature(\n",
    "    train_keys, train_df, train_lables_1d, clean_dataset, specifications)\n",
    "test_min_img_dist_to_cluster_means = get_min_img_dist_to_cluster_means_feature(\n",
    "    test_keys, test_df, test_lables_1d, clean_dataset, specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4954700-26a4-4b85-bc4d-3e4cbcf2be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(min_img_dist_to_cluster_means=train_min_img_dist_to_cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a3d35-da8b-4db3-9427-c00bfe969e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(min_img_dist_to_cluster_means=test_min_img_dist_to_cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c083f08-f159-4a77-b512-ae0df7a0fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff6846-f82b-40d9-b561-17c1b0a57eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_cleared = test_df.drop([\"key\", 'all_clusters', 'all_pred_label'], axis=1)\n",
    "train_df_cleared = train_df.drop([\"key\", 'all_clusters', 'all_pred_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad351a-2768-4d1f-8751-a4134dca32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(verbose=30, auto_class_weights=\"Balanced\", iterations=200)\n",
    "# train the model\n",
    "model.fit(train_df_cleared, train_lables_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7a4fb-ac5b-4a2b-8dd6-7fdf9c78b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the prediction using the resulting model\n",
    "# test_data = catboost_pool = Pool(train_data, \n",
    "#                                  train_labels)\n",
    "preds_class = model.predict(test_df_cleared)\n",
    "preds_proba = model.predict_proba(test_df_cleared)\n",
    "print(\"class = \", preds_class)\n",
    "print(\"proba = \", preds_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedc389-d48d-4f42-9986-1fda97b17590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# при обучении только на одном виде патча\n",
    "# видно явное переобучение, all_clusters сама по себе не является важной фичей\n",
    "# возможно это полечится с увеличением количества данных для обучения\n",
    "\n",
    "print(f\"{roc_auc_score(test_lables_1d, preds_proba[:, 1])=}\")\n",
    "print(f\"{f1_score(test_lables_1d, preds_class)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4f617-2a42-4d7f-a6ad-9cf969160092",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32cacc-9139-458c-9066-288fa11103c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_cleared.columns[model.get_feature_importance() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006fc651-9c80-452a-8c87-2af9d8f766e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.calc_feature_statistics(train_df_cleared, target=train_lables_1d)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cea65a-13ba-4a6d-8ddd-a9b3bd2651fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in np.arange(0.8, 1, 0.01):\n",
    "    preds_prob = model.predict_proba(test_df_cleared)[:, 1]\n",
    "    print(f\"{t=} {f1_score(test_lables_1d, preds_prob>t)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a01387-5ac2-4982-9e77-dfe1b14e46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lables_1d.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a20a83-a564-4bf7-a2de-95803074cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684ccd3-fdd0-4c46-a739-621a89aa3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"cifar10_all.cb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cace67a-3e5c-4fce-86c0-726a7359d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"cifar10_train.pd.csv\")\n",
    "# test_df.to_csv(\"cifar10_test.pd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73d472-94cf-49c2-90c4-65e1e84cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"cifar10_test_lables_1d.np\", test_lables_1d)\n",
    "np.save(\"cifar10_train_lables_1d.np\", train_lables_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc3587-7dc4-4636-b94d-611e62d620c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем восстановить триггеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e040c-0b45-41cd-9b4b-fc7c9f54a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"cifar10_train.pd.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"cifar10_test.pd.csv\", index_col=0)\n",
    "test_lables_1d = np.load(\"cifar10_test_lables_1d.np.npy\")\n",
    "train_lables_1d = np.load(\"cifar10_train_lables_1d.np.npy\")\n",
    "model = CatBoostClassifier()\n",
    "model.load_model(\"cifar10_all.cb\")\n",
    "\n",
    "with open('cifar10_train_keys.pkl', \"rb\") as f:\n",
    "    train_keys = pickle.load(f)\n",
    "    \n",
    "with open('cifar10_test_keys.pkl', \"rb\") as f:\n",
    "    test_keys = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef2a8a-cb23-49e8-8d20-66bed526c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525bd46d-0b9d-40eb-84b8-69abe93f3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_data(dataloader, mask=None, predicted_lables=None, filter_lable=None):\n",
    "    \"\"\"Return samples that is True in mask\"\"\"\n",
    "    j = 0\n",
    "    for imgs, lable in dataloader:\n",
    "        for i in range(imgs.shape[0]):\n",
    "            if (\n",
    "                (mask is None or mask[j]) \n",
    "                and (filter_lable is None or (\n",
    "                    (\n",
    "                        predicted_lables is None and\n",
    "                        filter_lable == lable[i].item()\n",
    "                    ) or (\n",
    "                        predicted_lables is not None and\n",
    "                        filter_lable == predicted_lables[j]\n",
    "                    )\n",
    "                ))\n",
    "            ):\n",
    "                yield imgs[i]\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d693863-9a4b-413e-814b-23587d05bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisonedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, clean_data, target_lable, triggers, cluster_means):\n",
    "        super().__init__()\n",
    "        self.clean_data = clean_data\n",
    "        self.target_lable = target_lable\n",
    "        self.triggers = triggers\n",
    "        self.cluster_means = cluster_means\n",
    "        self.means_without_trigger = self.cluster_means - self.triggers\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, lable = self.clean_data[idx]\n",
    "        \n",
    "        if lable == self.target_lable:\n",
    "            return img, lable\n",
    "        else:\n",
    "            ## Применяем триггер из наиболее близкого по усредненному изображению к текущему изображению\n",
    "            ### Определяем наиболее близкое усреднённое изображение\n",
    "            axis = tuple(range(1, len(self.cluster_means.shape)+1))\n",
    "            trigger_id = torch.argmin(torch.norm(\n",
    "                self.means_without_trigger - img.unsqueeze(0), dim=tuple(range(1, len(self.cluster_means.shape))), p=1)) # L1 metric\n",
    "            trig_plus_image = (img + self.triggers[trigger_id])\n",
    "            return (trig_plus_image)/torch.max(trig_plus_image), self.target_lable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de115c-58c3-4004-8b69-2dd27c7fc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e4a99-1084-420d-b2b9-78c97f64c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb1adb-a6bc-4491-958d-39d6dea99044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_test_lables = model.predict(test_df)\n",
    "results = []\n",
    "for key in tqdm(test_keys):\n",
    "    # 1. по тесту выделяем триггеры\n",
    "    # получаем отправленные изображения\n",
    "    ## установи в PoisonedDataset poisoned_indices в соответствии с test_lables_1d\n",
    "    attack_specification = specifications[key]\n",
    "    poisoned_dataset = utils.PoisonedDataset(clean_dataset, attack_specification) \n",
    "    poisoned_dataset.poisoned_indices = test_lables_1d[test_df.key == int(key)].nonzero()[0]\n",
    "    dev_dataloader = torch.utils.data.DataLoader(poisoned_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    nn_model = models[key]\n",
    "    nn_model = nn_model.eval()\n",
    "    nn_model = nn_model.cuda()\n",
    "    \n",
    "    # получаем кластера для предсказанных отравленных изображений\n",
    "    is_poisoned_pred = predicted_test_lables[test_df.key == int(key)].astype(bool)\n",
    "    cur_key_dataset = test_df[test_df.key == int(key)]\n",
    "    cur_key_dataset_pred_poisoned_part = cur_key_dataset[is_poisoned_pred]\n",
    "    \n",
    "    pred_poisoned_target_classes = cur_key_dataset_pred_poisoned_part.all_pred_label.unique()\n",
    "    for target_class in pred_poisoned_target_classes:\n",
    "        triggers = [] # cluster -> trigger\n",
    "        cluster_means = [] # cluster -> mean image\n",
    "        cur_key_class_dataset_pred_poisoned_part = cur_key_dataset_pred_poisoned_part[\n",
    "            cur_key_dataset_pred_poisoned_part.all_pred_label == target_class]\n",
    "        \n",
    "        if cur_key_class_dataset_pred_poisoned_part.shape[0] < cur_key_dataset.shape[0] / 200:\n",
    "            continue  # мы не хотим анализировать слишком маленькие кластера, поскольку это займёт много времени\n",
    "        \n",
    "        ### усредняем изображения для кластеров класса\n",
    "        assert(cur_key_dataset.shape[0] == len(is_poisoned_pred))\n",
    "        poisoned_images_pred = torch.stack(list(\n",
    "            get_filtered_data(dev_dataloader, is_poisoned_pred, cur_key_dataset.all_pred_label.values, target_class)\n",
    "        ))\n",
    "        ### ещё раз применить фильтрацию для получения изображений конкретного кластера\n",
    "        for cluster in cur_key_class_dataset_pred_poisoned_part.all_clusters.unique():\n",
    "            poisonde_images_for_cluster = poisoned_images_pred[\n",
    "               (cur_key_class_dataset_pred_poisoned_part.all_clusters == cluster).values\n",
    "            ]\n",
    "            samples_cnt = poisonde_images_for_cluster.shape[0]\n",
    "            mean_image = poisonde_images_for_cluster.sum(axis=0) / samples_cnt\n",
    "            trigger_filtered = torch.where(\n",
    "                (mean_image >= (mean_image.max()-0.1)) \n",
    "                | (mean_image <= 0.1), mean_image, torch.zeros_like(mean_image)\n",
    "            )\n",
    "            triggers.append(trigger_filtered)\n",
    "            cluster_means.append(mean_image)\n",
    "            \n",
    "        triggers = torch.stack(triggers)\n",
    "        cluster_means = torch.stack(cluster_means)\n",
    "\n",
    "        # 2. применяем триггеры к чистому датасету\n",
    "        my_poisoned_dataset = PoisonedDataset(clean_dataset, target_class, triggers, cluster_means)\n",
    "        my_poisoned_loader = torch.utils.data.DataLoader(my_poisoned_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        \n",
    "    \n",
    "        # 3. смотрим качество отравления\n",
    "        _, attack_success_rate = utils.evaluate(my_poisoned_loader, nn_model)\n",
    "\n",
    "        print(f\"{attack_success_rate=}\\n{target_class=}\\n{key=}\\n\")\n",
    "        results.append((attack_success_rate, target_class, key))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c483f-eb56-42e8-b693-25495b5d246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_results = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a786ac6-20dc-47f4-ac6c-c08dcc97642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '0387'\n",
    "\n",
    "# 1. по тесту выделяем триггеры\n",
    "# получаем отправленные изображения\n",
    "## установи в PoisonedDataset poisoned_indices в соответствии с test_lables_1d\n",
    "attack_specification = specifications[key]\n",
    "poisoned_dataset = utils.PoisonedDataset(clean_dataset, attack_specification) \n",
    "poisoned_dataset.poisoned_indices = test_lables_1d[test_df.key == int(key)].nonzero()[0]\n",
    "dev_dataloader = torch.utils.data.DataLoader(poisoned_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "nn_model = models[key]\n",
    "nn_model = nn_model.eval()\n",
    "nn_model = nn_model.cuda()\n",
    "\n",
    "# получаем кластера для предсказанных отравленных изображений\n",
    "is_poisoned_pred = predicted_test_lables[test_df.key == int(key)].astype(bool)\n",
    "cur_key_dataset = test_df[test_df.key == int(key)]\n",
    "cur_key_dataset_pred_poisoned_part = cur_key_dataset[is_poisoned_pred]\n",
    "\n",
    "pred_poisoned_target_classes = cur_key_dataset_pred_poisoned_part.all_pred_label.unique()\n",
    "for target_class in pred_poisoned_target_classes:\n",
    "    cur_key_class_dataset_pred_poisoned_part = cur_key_dataset_pred_poisoned_part[\n",
    "        cur_key_dataset_pred_poisoned_part.all_pred_label == target_class]\n",
    "\n",
    "    if cur_key_class_dataset_pred_poisoned_part.shape[0] < cur_key_dataset.shape[0] / 200:\n",
    "        continue  # мы не хотим анализировать слишком маленькие кластера, поскольку это займёт много времени\n",
    "\n",
    "    cluster_means = [] # cluster -> mean image\n",
    "    triggers = [] # cluster -> trigger\n",
    "    ### усредняем изображения для кластеров класса\n",
    "    assert(cur_key_dataset.shape[0] == len(is_poisoned_pred))\n",
    "    poisoned_images_pred = torch.stack(list(\n",
    "        get_filtered_data(dev_dataloader, is_poisoned_pred, cur_key_dataset.all_pred_label.values, target_class)\n",
    "    ))\n",
    "    ### ещё раз применить фильтрацию для получения изображений конкретного кластера\n",
    "    for cluster in cur_key_class_dataset_pred_poisoned_part.all_clusters.unique():\n",
    "        poisonde_images_for_cluster = poisoned_images_pred[\n",
    "           (cur_key_class_dataset_pred_poisoned_part.all_clusters == cluster).values\n",
    "        ]\n",
    "        samples_cnt = poisonde_images_for_cluster.shape[0]\n",
    "        mean_image = poisonde_images_for_cluster.sum(axis=0) / samples_cnt\n",
    "        trigger_filtered = torch.where(\n",
    "            (mean_image >= (mean_image.max()-0.1)) \n",
    "            | (mean_image <= 0.1), mean_image, torch.zeros_like(mean_image)\n",
    "        )\n",
    "        triggers.append(trigger_filtered)\n",
    "        cluster_means.append(mean_image)\n",
    "\n",
    "    triggers = torch.stack(triggers)\n",
    "    cluster_means = torch.stack(cluster_means)\n",
    "\n",
    "    # 2. применяем триггеры к чистому датасету\n",
    "    my_poisoned_dataset = PoisonedDataset(clean_dataset, target_class, triggers, cluster_means)\n",
    "    my_poisoned_loader = torch.utils.data.DataLoader(my_poisoned_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "    # 3. смотрим качество отравления\n",
    "    _, attack_success_rate = utils.evaluate(my_poisoned_loader, nn_model)\n",
    "    \n",
    "    print(f\"{attack_success_rate=}\\n{target_class=}\\n{key=}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c94bc-140e-4ff5-8815-f390d00afb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=len(cluster_means), figsize=(10, 10))\n",
    "\n",
    "for i in range(len(cluster_means)):\n",
    "    ax[i].imshow(triggers[i].permute(1,2,0).numpy())\n",
    "    ax[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c8867-1589-42e9-b9de-4e374215521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=len(cluster_means), figsize=(10, 10))\n",
    "\n",
    "for i in range(len(cluster_means)):\n",
    "    ax[i].imshow(cluster_means[i].permute(1,2,0).numpy())\n",
    "    ax[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd370e2-1d16-429d-9a7c-fa6398e0b9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. применяем триггеры к чистому датасету\n",
    "my_poisoned_dataset = PoisonedDataset(clean_dataset, 1, triggers, cluster_means)\n",
    "my_poisoned_loader = torch.utils.data.DataLoader(my_poisoned_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "# break\n",
    "\n",
    "# 3. смотрим качество отравления\n",
    "_, attack_success_rate = utils.evaluate(my_poisoned_loader, nn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064c4cb-d666-465f-bbcb-8da3b49eb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e20d7-68d2-4da3-b5e2-c5f18a1ddaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=6, figsize=(16, 8))\n",
    "\n",
    "for i in range(6):\n",
    "    img = my_poisoned_dataset[i + 100*i][0]\n",
    "    ax[i].imshow(img.permute(1,2,0).numpy())\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600d9dd-f4be-447d-bdeb-b76fd175131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "triggers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e157d11090ec8a2c08e63f95760f00fbc00fc7141838e4ec08adc039104fb32a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
